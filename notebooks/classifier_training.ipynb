{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "spark = SparkSession.builder.appName('recommend-ML').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+--------+-----------+------------+----------+---------+---------+---------+-----------------+------------------+------------+\n",
      "|UserId|TrackId|AlbumId|ArtistId|AlbumRating|ArtistRating|TotalScore|Predictor|MinRating|MaxRating|       MeanRating|          Variance|MedianRating|\n",
      "+------+-------+-------+--------+-----------+------------+----------+---------+---------+---------+-----------------+------------------+------------+\n",
      "|200031|  30877| 192723|  132319|       90.0|        50.0|     140.0|        1|        0|      100|53.42664670658683|1195.6428228871598|        60.0|\n",
      "|200031|   8244| 223220|  233697|       90.0|         0.0|      90.0|        1|        0|      100|55.75752773375594|1216.3168014948726|        70.0|\n",
      "|200031| 130183|   None|    None|        0.0|         0.0|       0.0|        0|        0|       90|57.69230769230769| 1063.905325443787|        70.0|\n",
      "|200031| 198762| 220103|  113265|        0.0|         0.0|       0.0|        0|        0|      100|68.88888888888889| 1017.283950617284|        90.0|\n",
      "|200031|  34503|  43738|  173170|       90.0|        50.0|     140.0|        1|        0|      100|48.66293706293706|1197.5675074575774|        50.0|\n",
      "+------+-------+-------+--------+-----------+------------+----------+---------+---------+---------+-----------------+------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import test_2 ground truths\n",
    "data_dir = \"../data/proccessed/new.csv\"\n",
    "df = spark.read.csv(data_dir, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"UserId\", df[\"UserId\"].cast(IntegerType())) \\\n",
    "    .withColumn(\"TrackId\", df[\"TrackId\"].cast(IntegerType())) \\\n",
    "    .withColumn(\"AlbumId\", df[\"AlbumId\"].cast(IntegerType())) \\\n",
    "    .withColumn(\"ArtistId\", df[\"ArtistId\"].cast(IntegerType())) \\\n",
    "    .withColumn(\"AlbumRating\", df[\"AlbumRating\"].cast(FloatType())) \\\n",
    "    .withColumn(\"ArtistRating\", df[\"ArtistRating\"].cast(FloatType())) \\\n",
    "    .withColumn(\"TotalScore\", df[\"TotalScore\"].cast(FloatType())) \\\n",
    "    .withColumn(\"Predictor\", df[\"Predictor\"].cast(IntegerType())) \\\n",
    "    .withColumn(\"MinRating\", df[\"MinRating\"].cast(FloatType())) \\\n",
    "    .withColumn(\"MaxRating\", df[\"MaxRating\"].cast(FloatType())) \\\n",
    "    .withColumn(\"MeanRating\", df[\"MeanRating\"].cast(FloatType())) \\\n",
    "    .withColumn(\"Variance\", df[\"Variance\"].cast(FloatType())) \\\n",
    "    .withColumn(\"MedianRating\", df[\"MedianRating\"].cast(FloatType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-------+-------+--------+-----------+------------+----------+---------+---------+---------+----------+----------+------------+\n",
      "|            features|UserId|TrackId|AlbumId|ArtistId|AlbumRating|ArtistRating|TotalScore|Predictor|MinRating|MaxRating|MeanRating|  Variance|MedianRating|\n",
      "+--------------------+------+-------+-------+--------+-----------+------------+----------+---------+---------+---------+----------+----------+------------+\n",
      "|[30877.0,192723.0...|200031|  30877| 192723|  132319|       90.0|        50.0|     140.0|        1|      0.0|    100.0| 53.426647| 1195.6428|        60.0|\n",
      "|[8244.0,223220.0,...|200031|   8244| 223220|  233697|       90.0|         0.0|      90.0|        1|      0.0|    100.0| 55.757526| 1216.3168|        70.0|\n",
      "|(11,[0,7,8,9,10],...|200031| 130183|      0|       0|        0.0|         0.0|       0.0|        0|      0.0|     90.0| 57.692307| 1063.9053|        70.0|\n",
      "|[198762.0,220103....|200031| 198762| 220103|  113265|        0.0|         0.0|       0.0|        0|      0.0|    100.0| 68.888885|1017.28394|        90.0|\n",
      "|[34503.0,43738.0,...|200031|  34503|  43738|  173170|       90.0|        50.0|     140.0|        1|      0.0|    100.0| 48.662937| 1197.5675|        50.0|\n",
      "|[227283.0,183931....|200031| 227283| 183931|  270460|        0.0|        90.0|      90.0|        0|      0.0|    100.0| 64.072845|1003.48474|        70.0|\n",
      "|[218377.0,94189.0...|200032| 218377|  94189|  178617|        0.0|         0.0|       0.0|        0|      0.0|     90.0|      69.0|     849.0|        90.0|\n",
      "|[110262.0,122655....|200032| 110262| 122655|   52340|        0.0|         0.0|       0.0|        0|      0.0|    100.0|  56.82427| 1080.8562|        70.0|\n",
      "|[18681.0,263622.0...|200032|  18681| 263622|  211701|       90.0|        90.0|     180.0|        1|      0.0|     90.0| 23.333334| 1222.2222|         0.0|\n",
      "|[138493.0,58759.0...|200032| 138493|  58759|  182642|       90.0|        90.0|     180.0|        1|      0.0|    100.0|  62.44898|  957.2678|        70.0|\n",
      "|[64167.0,202904.0...|200032|  64167| 202904|   46490|       90.0|        90.0|     180.0|        1|      0.0|    100.0| 50.714287| 1531.6327|        50.0|\n",
      "|[22820.0,15669.0,...|200032|  22820|  15669|  103665|        0.0|         0.0|       0.0|        0|      0.0|    100.0|      69.0|  711.8571|        70.0|\n",
      "|[52198.0,209712.0...|200055|  52198| 209712|  269659|       90.0|        90.0|     180.0|        1|      0.0|    100.0| 64.638885| 1511.1196|        88.0|\n",
      "|[233815.0,280805....|200055| 233815| 280805|  205738|        0.0|         0.0|       0.0|        0|      0.0|    100.0| 45.689655| 1514.1796|        50.0|\n",
      "|[175557.0,194035....|200055| 175557| 194035|  228477|        0.0|         0.0|       0.0|        0|      0.0|     90.0|      60.0|    1400.0|        90.0|\n",
      "|[59101.0,23848.0,...|200055|  59101|  23848|  159812|        0.0|         0.0|       0.0|        0|      0.0|    100.0| 41.666668| 1513.8889|        40.0|\n",
      "|[56695.0,209712.0...|200055|  56695| 209712|  269659|       90.0|        90.0|     180.0|        1|      0.0|    100.0| 46.636364| 1948.1405|        45.0|\n",
      "|[134398.0,156574....|200055| 134398| 156574|    9002|        0.0|        90.0|      90.0|        1|      0.0|    100.0|  70.90909| 862.80994|        70.0|\n",
      "|(11,[0,1,7,8,9,10...|200065| 179571| 178160|       0|        0.0|         0.0|       0.0|        0|      0.0|     90.0|      37.0|    1241.0|        35.0|\n",
      "|[196286.0,34281.0...|200065| 196286|  34281|  171349|        0.0|         0.0|       0.0|        0|      0.0|    100.0| 43.471832| 1546.8126|        50.0|\n",
      "+--------------------+------+-------+-------+--------+-----------+------------+----------+---------+---------+---------+----------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values with 0\n",
    "df = df.fillna(0, subset=['AlbumId', 'TrackId', 'ArtistId'])\n",
    "\n",
    "stages = []\n",
    "numericCols = ['TrackId', 'AlbumId', 'ArtistId',\n",
    "               \"AlbumRating\", \"ArtistRating\", \"TotalScore\", \"MinRating\", \"MaxRating\", \"MeanRating\", \"Variance\", \"MedianRating\"]\n",
    "assemblerInputs = numericCols \n",
    "assembler = VectorAssembler(\n",
    "    inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "# apply\n",
    "cols = df.columns\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df = pipelineModel.transform(df)\n",
    "selectedCols = ['features'] + cols\n",
    "df = df.select(selectedCols)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest User ID: 212234\n",
      "Lowest User ID: 200031\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, min\n",
    "\n",
    "# Calculate the highest and lowest user ID\n",
    "max_user_id = df.agg(max(\"UserId\")).collect()[0][0]\n",
    "min_user_id = df.agg(min(\"UserId\")).collect()[0][0]\n",
    "\n",
    "print(\"Highest User ID:\", max_user_id)\n",
    "print(\"Lowest User ID:\", min_user_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 4260\n",
      "Test Dataset Count: 1740\n"
     ]
    }
   ],
   "source": [
    "#train = df.where(col(\"UserId\").between(202000, 212234))\n",
    "#test = df.where(col(\"UserId\").between(200031, 201999))\n",
    "#print(\"Training Dataset Count: \" + str(train.count()))\n",
    "#print(\"Test Dataset Count: \" + str(test.count()))\n",
    "\n",
    "# below is the typical random split\n",
    "# of the train and test data sets\n",
    "# HOWEVER, our testing users have 6 tracks for each\n",
    "# We cannot use random split here\n",
    "train, test = df.randomSplit([0.7, 0.3], seed=2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------+--------------------+--------------------+----------+\n",
      "|UserId|TrackId|Predictor|         probability|       rawPrediction|prediction|\n",
      "+------+-------+---------+--------------------+--------------------+----------+\n",
      "|205246| 237713|        0|[0.91903502683263...|[2.42930760458673...|       0.0|\n",
      "|200625|   5281|        0|[0.79099615862614...|[1.33094047982947...|       0.0|\n",
      "|206067|   9408|        0|[0.79738898848394...|[1.37005468422200...|       0.0|\n",
      "|210889|  11423|        0|[0.79758407381363...|[1.37126262980322...|       0.0|\n",
      "|209063|  21778|        0|[0.83037296154063...|[1.58827281558701...|       0.0|\n",
      "|210234|  27337|        1|[0.81637607600320...|[1.49198535155676...|       0.0|\n",
      "|210746|  30548|        0|[0.81122267447125...|[1.45797443554659...|       0.0|\n",
      "|207658|  46693|        0|[0.87956492352701...|[1.98831655685312...|       0.0|\n",
      "|200166|  49989|        1|[0.84961383245739...|[1.73157549533631...|       0.0|\n",
      "|207499|  51434|        0|[0.77958272195764...|[1.26323633645825...|       0.0|\n",
      "|204905|  56166|        0|[0.82889566609911...|[1.57782078187285...|       0.0|\n",
      "|201314|  65650|        0|[0.87066689955721...|[1.90686821896002...|       0.0|\n",
      "+------+-------+---------+--------------------+--------------------+----------+\n",
      "only showing top 12 rows\n",
      "\n",
      "Model's areaUnderROC:  0.9018418976197794\n",
      "Coefficients: [-1.0312435282645766e-06,-2.234666573612535e-06,-2.0496875128672568e-06,0.022676565072485576,0.019546053245983283,0.012421136143102614,0.0008209216798986394,0.0016822279326263298,-0.0019657819851249844,0.00025323710031824553,0.003108263454630364]\n",
      "Intercept: -1.251451566802531\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='Predictor')\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .addGrid(lr.maxIter, [1, 5, 10])\n",
    "             .build())\n",
    "\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=BinaryClassificationEvaluator(\n",
    "                        labelCol='Predictor'),\n",
    "                    numFolds=5)\n",
    "\n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "predictions = cvModel.transform(test)\n",
    "predictions.select('UserId', 'TrackId', 'Predictor', 'probability',\n",
    "                   'rawPrediction', 'prediction').show(12)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol='Predictor', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "\n",
    "lr_bestModel = cvModel.bestModel\n",
    "\n",
    "print(\"Model's areaUnderROC: \", evaluator.evaluate(predictions))\n",
    "print(\"Coefficients: \" + str(cvModel.bestModel.coefficients))\n",
    "print(\"Intercept: \" + str(cvModel.bestModel.intercept))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------+--------------------+--------------+----------+\n",
      "|UserId|TrackId|Predictor|         probability| rawPrediction|prediction|\n",
      "+------+-------+---------+--------------------+--------------+----------+\n",
      "|205246| 237713|        0|[0.85233644859813...|[1824.0,316.0]|       0.0|\n",
      "|200625|   5281|        0|[0.85233644859813...|[1824.0,316.0]|       0.0|\n",
      "|206067|   9408|        0|[0.85233644859813...|[1824.0,316.0]|       0.0|\n",
      "|210889|  11423|        0|[0.85233644859813...|[1824.0,316.0]|       0.0|\n",
      "|209063|  21778|        0|[0.85233644859813...|[1824.0,316.0]|       0.0|\n",
      "|210234|  27337|        1|[0.85233644859813...|[1824.0,316.0]|       0.0|\n",
      "|210746|  30548|        0|[0.85233644859813...|[1824.0,316.0]|       0.0|\n",
      "|207658|  46693|        0|[0.85233644859813...|[1824.0,316.0]|       0.0|\n",
      "|200166|  49989|        1|[0.85233644859813...|[1824.0,316.0]|       0.0|\n",
      "|207499|  51434|        0|[0.85233644859813...|[1824.0,316.0]|       0.0|\n",
      "|204905|  56166|        0|[0.85233644859813...|[1824.0,316.0]|       0.0|\n",
      "|201314|  65650|        0|[0.85233644859813...|[1824.0,316.0]|       0.0|\n",
      "+------+-------+---------+--------------------+--------------+----------+\n",
      "only showing top 12 rows\n",
      "\n",
      "Best model's maxDepth:  5\n",
      "Best model's minInstancesPerNode:  5\n",
      "Best model's accuracy:  0.8551724137931035\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='Predictor')\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [5]) \\\n",
    "    .addGrid(dt.minInstancesPerNode, [5]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='Predictor', predictionCol='prediction', metricName='accuracy')\n",
    "\n",
    "crossval = CrossValidator(estimator=dt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "predictions = cvModel.transform(test)\n",
    "predictions.select('UserId', 'TrackId', 'Predictor', 'probability',\n",
    "                   'rawPrediction', 'prediction').show(12)\n",
    "\n",
    "dt_bestModel = cvModel.bestModel\n",
    "print(\"Best model's maxDepth: \", dt_bestModel._java_obj.getMaxDepth())\n",
    "print(\"Best model's minInstancesPerNode: \",\n",
    "      dt_bestModel._java_obj.getMinInstancesPerNode())\n",
    "print(\"Best model's accuracy: \", evaluator.evaluate(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------+--------------------+--------------------+----------+\n",
      "|UserId|TrackId|Predictor|         probability|       rawPrediction|prediction|\n",
      "+------+-------+---------+--------------------+--------------------+----------+\n",
      "|205246| 237713|        0|[0.84589579945532...|[4.22947899727661...|       0.0|\n",
      "|200625|   5281|        0|[0.84589579945532...|[4.22947899727661...|       0.0|\n",
      "|206067|   9408|        0|[0.84589579945532...|[4.22947899727661...|       0.0|\n",
      "|210889|  11423|        0|[0.84589579945532...|[4.22947899727661...|       0.0|\n",
      "|209063|  21778|        0|[0.84589579945532...|[4.22947899727661...|       0.0|\n",
      "|210234|  27337|        1|[0.84589579945532...|[4.22947899727661...|       0.0|\n",
      "|210746|  30548|        0|[0.84589579945532...|[4.22947899727661...|       0.0|\n",
      "|207658|  46693|        0|[0.84589579945532...|[4.22947899727661...|       0.0|\n",
      "|200166|  49989|        1|[0.84589579945532...|[4.22947899727661...|       0.0|\n",
      "|207499|  51434|        0|[0.84589579945532...|[4.22947899727661...|       0.0|\n",
      "|204905|  56166|        0|[0.84589579945532...|[4.22947899727661...|       0.0|\n",
      "|201314|  65650|        0|[0.84589579945532...|[4.22947899727661...|       0.0|\n",
      "+------+-------+---------+--------------------+--------------------+----------+\n",
      "only showing top 12 rows\n",
      "\n",
      "Best model's numTrees:  5\n",
      "Best model's maxDepth:  3\n",
      "Best model's impurity:  gini\n",
      "Best model's accuracy:  0.8563218390804598\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='Predictor')\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [2,3,5]) \\\n",
    "    .addGrid(rf.maxDepth, [1,2,3]) \\\n",
    "    .addGrid(rf.impurity, ['gini', 'entropy']) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='Predictor', predictionCol='prediction', metricName='accuracy')\n",
    "\n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "predictions = cvModel.transform(test)\n",
    "predictions.select('UserId', 'TrackId', 'Predictor', 'probability',\n",
    "                   'rawPrediction', 'prediction').show(12)\n",
    "\n",
    "rf_bestModel = cvModel.bestModel\n",
    "print(\"Best model's numTrees: \", rf_bestModel._java_obj.getNumTrees())\n",
    "print(\"Best model's maxDepth: \", rf_bestModel._java_obj.getMaxDepth())\n",
    "print(\"Best model's impurity: \", rf_bestModel._java_obj.getImpurity())\n",
    "print(\"Best model's accuracy: \", evaluator.evaluate(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------+--------------------+--------------------+----------+\n",
      "|UserId|TrackId|Predictor|         probability|       rawPrediction|prediction|\n",
      "+------+-------+---------+--------------------+--------------------+----------+\n",
      "|205246| 237713|        0|[0.85401181151936...|[0.88320965344686...|       0.0|\n",
      "|200625|   5281|        0|[0.88784061484277...|[1.03443564882892...|       0.0|\n",
      "|206067|   9408|        0|[0.86202780132885...|[0.91611765815416...|       0.0|\n",
      "|210889|  11423|        0|[0.71433146454814...|[0.45825745946521...|       0.0|\n",
      "|209063|  21778|        0|[0.88784061484277...|[1.03443564882892...|       0.0|\n",
      "|210234|  27337|        1|[0.79017076092421...|[0.66297751124812...|       0.0|\n",
      "|210746|  30548|        0|[0.89428072877188...|[1.06761627193538...|       0.0|\n",
      "|207658|  46693|        0|[0.85264043425887...|[0.87773115114081...|       0.0|\n",
      "|200166|  49989|        1|[0.81931601363027...|[0.75586014438376...|       0.0|\n",
      "|207499|  51434|        0|[0.88784061484277...|[1.03443564882892...|       0.0|\n",
      "|204905|  56166|        0|[0.70080894400315...|[0.42557647475373...|       0.0|\n",
      "|201314|  65650|        0|[0.81832178507309...|[0.75250928357711...|       0.0|\n",
      "+------+-------+---------+--------------------+--------------------+----------+\n",
      "only showing top 12 rows\n",
      "\n",
      "Best model's maxIter:  10\n",
      "Best model's maxDepth:  5\n",
      "Best model's stepSize:  0.1\n",
      "Best model's accuracy:  0.8545977011494252\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "gbt = GBTClassifier(featuresCol='features', labelCol='Predictor')\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [10]) \\\n",
    "    .addGrid(gbt.maxDepth, [5]) \\\n",
    "    .addGrid(gbt.stepSize, [0.1]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='Predictor', predictionCol='prediction', metricName='accuracy')\n",
    "\n",
    "crossval = CrossValidator(estimator=gbt,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "predictions = cvModel.transform(test)\n",
    "predictions.select('UserId', 'TrackId', 'Predictor', 'probability',\n",
    "                   'rawPrediction', 'prediction').show(12)\n",
    "\n",
    "gbt_bestModel = cvModel.bestModel\n",
    "print(\"Best model's maxIter: \", gbt_bestModel._java_obj.getMaxIter())\n",
    "print(\"Best model's maxDepth: \", gbt_bestModel._java_obj.getMaxDepth())\n",
    "print(\"Best model's stepSize: \", gbt_bestModel._java_obj.getStepSize())\n",
    "print(\"Best model's accuracy: \", evaluator.evaluate(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lr = lr_bestModel.transform(test)\n",
    "\n",
    "predictions_dt = dt_bestModel.transform(test)\n",
    "\n",
    "predictions_rf = rf_bestModel.transform(test)\n",
    "\n",
    "predictions_gbt = gbt_bestModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------+-------------+-------------+-------------+--------------+\n",
      "|UserId|TrackId|Predictor|lr_prediction|dt_prediction|rf_prediction|gbt_prediction|\n",
      "+------+-------+---------+-------------+-------------+-------------+--------------+\n",
      "|205246| 237713|        0|          0.0|          0.0|          0.0|           0.0|\n",
      "|200625|   5281|        0|          0.0|          0.0|          0.0|           0.0|\n",
      "|206067|   9408|        0|          0.0|          0.0|          0.0|           0.0|\n",
      "|210889|  11423|        0|          0.0|          0.0|          0.0|           0.0|\n",
      "|209063|  21778|        0|          0.0|          0.0|          0.0|           0.0|\n",
      "|210234|  27337|        1|          0.0|          0.0|          0.0|           0.0|\n",
      "|210746|  30548|        0|          0.0|          0.0|          0.0|           0.0|\n",
      "|207658|  46693|        0|          0.0|          0.0|          0.0|           0.0|\n",
      "|200166|  49989|        1|          0.0|          0.0|          0.0|           0.0|\n",
      "|207499|  51434|        0|          0.0|          0.0|          0.0|           0.0|\n",
      "|204905|  56166|        0|          0.0|          0.0|          0.0|           0.0|\n",
      "|201314|  65650|        0|          0.0|          0.0|          0.0|           0.0|\n",
      "+------+-------+---------+-------------+-------------+-------------+--------------+\n",
      "only showing top 12 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "predictions_lr = predictions_lr.withColumnRenamed(\n",
    "    \"prediction\", \"lr_prediction\")\n",
    "predictions_dt = predictions_dt.withColumnRenamed(\n",
    "    \"prediction\", \"dt_prediction\")\n",
    "predictions_rf = predictions_rf.withColumnRenamed(\n",
    "    \"prediction\", \"rf_prediction\")\n",
    "predictions_gbt = predictions_gbt.withColumnRenamed(\n",
    "    \"prediction\", \"gbt_prediction\")\n",
    "\n",
    "\n",
    "combined_df = predictions_lr.select(\"UserId\", \"TrackId\", \"Predictor\", \"lr_prediction\") \\\n",
    "    .join(predictions_dt.select(\"UserId\", \"TrackId\", \"dt_prediction\"), [\"UserId\", \"TrackId\"]) \\\n",
    "    .join(predictions_rf.select(\"UserId\", \"TrackId\", \"rf_prediction\"), [\"UserId\", \"TrackId\"]) \\\n",
    "    .join(predictions_gbt.select(\"UserId\", \"TrackId\", \"gbt_prediction\"), [\"UserId\", \"TrackId\"])\n",
    "\n",
    "combined_df.show(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/10 18:22:51 WARN Instrumentation: [735af279] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.3322246852290674,0.6629075455680206,0.14856100432541708,-0.4030910378694768]\n",
      "Intercept: 0.18853145010668956\n",
      "Root Mean Squared Error (RMSE) on test data = 0.381317\n",
      "Accuracy = 0.854598\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Create a VectorAssembler which consumes columns lr_prediction, dt_prediction, rf_prediction and gbt_prediction and produces a new column \"features\"\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "                            \"lr_prediction\", \"dt_prediction\", \"rf_prediction\", \"gbt_prediction\"], outputCol=\"features\")\n",
    "\n",
    "# Use the assembler to transform our DataFrame to the two-column format\n",
    "df = assembler.transform(combined_df)\n",
    "\n",
    "# Initialize LinearRegression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Predictor\")\n",
    "\n",
    "# Fit the model to the data\n",
    "ensemble_lr = lr.fit(df)\n",
    "\n",
    "\n",
    "print(\"Coefficients: \" + str(ensemble_lr.coefficients))\n",
    "print(\"Intercept: \" + str(ensemble_lr.intercept))\n",
    "\n",
    "\n",
    "df = ensemble_lr.transform(df)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Predictor\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "predictions = predictions.withColumn(\n",
    "    \"final_prediction\", (col(\"prediction\") > 0.5).cast(\"double\"))\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Predictor\", predictionCol=\"final_prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+--------+-----------+------------+----------+---------+---------+------------------+------------------+------------+\n",
      "|UserId|TrackId|AlbumId|ArtistId|AlbumRating|ArtistRating|TotalScore|MinRating|MaxRating|        MeanRating|          Variance|MedianRating|\n",
      "+------+-------+-------+--------+-----------+------------+----------+---------+---------+------------------+------------------+------------+\n",
      "|199810| 208019| 209288|    None|        0.0|         0.0|       0.0|        0|      100|49.766129032258064|1349.9533688865763|        50.0|\n",
      "|199810|  74139| 277282|  271146|        0.0|         0.0|       0.0|       50|       90| 78.33333333333333|297.22222222222223|        90.0|\n",
      "|199810|   9903|   None|    None|        0.0|         0.0|       0.0|        0|      100|52.858823529411765|1339.4977162629757|        50.0|\n",
      "|199810| 242681| 190640|  244574|        0.0|         0.0|       0.0|        0|      100| 49.50834597875569|1692.7537239713458|        50.0|\n",
      "|199810|  18515| 146344|   33168|        0.0|        70.0|      70.0|        0|      100|  55.1566265060241|1373.2586006677311|        70.0|\n",
      "+------+-------+-------+--------+-----------+------------+----------+---------+---------+------------------+------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import test_2 ground truths\n",
    "data_dir = \"../data/proccessed/test.csv\"\n",
    "test_df = spark.read.csv(data_dir, header=True, inferSchema=True)\n",
    "test_df.show(5)\n",
    "\n",
    "test_df = test_df.withColumn(\"UserId\", test_df[\"UserId\"].cast(IntegerType())) \\\n",
    "    .withColumn(\"TrackId\", test_df[\"TrackId\"].cast(IntegerType())) \\\n",
    "    .withColumn(\"AlbumId\", test_df[\"AlbumId\"].cast(IntegerType())) \\\n",
    "    .withColumn(\"ArtistId\", test_df[\"ArtistId\"].cast(IntegerType())) \\\n",
    "    .withColumn(\"AlbumRating\", test_df[\"AlbumRating\"].cast(FloatType())) \\\n",
    "    .withColumn(\"ArtistRating\", test_df[\"ArtistRating\"].cast(FloatType())) \\\n",
    "    .withColumn(\"TotalScore\", test_df[\"TotalScore\"].cast(FloatType())) \\\n",
    "    .withColumn(\"MinRating\", test_df[\"MinRating\"].cast(FloatType())) \\\n",
    "    .withColumn(\"MaxRating\", test_df[\"MaxRating\"].cast(FloatType())) \\\n",
    "    .withColumn(\"MeanRating\", test_df[\"MeanRating\"].cast(FloatType())) \\\n",
    "    .withColumn(\"Variance\", test_df[\"Variance\"].cast(FloatType())) \\\n",
    "    .withColumn(\"MedianRating\", test_df[\"MedianRating\"].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-------+-------+--------+-----------+------------+----------+---------+---------+----------+---------+------------+\n",
      "|            features|UserId|TrackId|AlbumId|ArtistId|AlbumRating|ArtistRating|TotalScore|MinRating|MaxRating|MeanRating| Variance|MedianRating|\n",
      "+--------------------+------+-------+-------+--------+-----------+------------+----------+---------+---------+----------+---------+------------+\n",
      "|(11,[0,1,7,8,9,10...|199810| 208019| 209288|       0|        0.0|         0.0|       0.0|      0.0|    100.0|  49.76613|1349.9534|        50.0|\n",
      "|[74139.0,277282.0...|199810|  74139| 277282|  271146|        0.0|         0.0|       0.0|     50.0|     90.0| 78.333336|297.22223|        90.0|\n",
      "|(11,[0,7,8,9,10],...|199810|   9903|      0|       0|        0.0|         0.0|       0.0|      0.0|    100.0|  52.85882|1339.4977|        50.0|\n",
      "|[242681.0,190640....|199810| 242681| 190640|  244574|        0.0|         0.0|       0.0|      0.0|    100.0| 49.508347|1692.7538|        50.0|\n",
      "|[18515.0,146344.0...|199810|  18515| 146344|   33168|        0.0|        70.0|      70.0|      0.0|    100.0| 55.156628|1373.2585|        70.0|\n",
      "|[105760.0,93458.0...|199810| 105760|  93458|   11616|        0.0|        90.0|      90.0|      0.0|     90.0| 63.652172|761.09644|        70.0|\n",
      "|[276940.0,201356....|199812| 276940| 201356|  163237|        0.0|         0.0|       0.0|      0.0|     70.0| 31.666666| 847.2222|        30.0|\n",
      "|[142408.0,112725....|199812| 142408| 112725|  275191|      100.0|       100.0|     200.0|      0.0|    100.0| 55.712143|1595.0656|        70.0|\n",
      "|[130023.0,226816....|199812| 130023| 226816|  275191|      100.0|       100.0|     200.0|      0.0|    100.0|  57.87394|1575.8949|        70.0|\n",
      "|[29189.0,104694.0...|199812|  29189| 104694|  211701|        0.0|         0.0|       0.0|      0.0|    100.0|   51.8228|1697.0487|        60.0|\n",
      "|[223706.0,101750....|199812| 223706| 101750|  128069|        0.0|       100.0|     100.0|      0.0|    100.0| 64.650604|1532.7334|        90.0|\n",
      "|[211361.0,127464....|199812| 211361| 127464|   19438|        0.0|         0.0|       0.0|      0.0|    100.0| 60.588234|1476.1245|        90.0|\n",
      "|[188441.0,195380....|199813| 188441| 195380|   22935|        0.0|        90.0|      90.0|      0.0|    100.0| 43.958332|1311.4149|        50.0|\n",
      "|(11,[0,7,8,9,10],...|199813|  20968|      0|       0|        0.0|         0.0|       0.0|      0.0|    100.0|  60.21698|1151.2982|        70.0|\n",
      "|[21571.0,217069.0...|199813|  21571| 217069|   89183|       90.0|        90.0|     180.0|      0.0|     90.0|  61.11111| 809.8765|        50.0|\n",
      "|[79640.0,43214.0,...|199813|  79640|  43214|  232859|        0.0|        90.0|      90.0|      0.0|    100.0| 61.962265|1043.6967|        70.0|\n",
      "|[184173.0,292343....|199813| 184173| 292343|  196404|        0.0|        70.0|      70.0|      0.0|    100.0|  45.98901|1249.2966|        50.0|\n",
      "|[111874.0,117381....|199813| 111874| 117381|  231280|        0.0|         0.0|       0.0|      0.0|    100.0| 52.119404|1254.9011|        50.0|\n",
      "|[122375.0,99568.0...|199814| 122375|  99568|  105364|        0.0|         0.0|       0.0|      0.0|    100.0| 55.533333|1518.5277|        70.0|\n",
      "|[189043.0,9481.0,...|199814| 189043|   9481|  247092|       75.0|        75.0|     150.0|      0.0|    100.0| 48.520756| 1455.593|        50.0|\n",
      "+--------------------+------+-------+-------+--------+-----------+------------+----------+---------+---------+----------+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values with 0\n",
    "test_df = test_df.fillna(0, subset=['AlbumId', 'TrackId', 'ArtistId'])\n",
    "\n",
    "stages = []\n",
    "numericCols = ['TrackId', 'AlbumId', 'ArtistId',\n",
    "               \"AlbumRating\", \"ArtistRating\", \"TotalScore\", \"MinRating\", \"MaxRating\", \"MeanRating\", \"Variance\", \"MedianRating\"]\n",
    "assemblerInputs = numericCols\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "# apply\n",
    "cols = test_df.columns\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(test_df)\n",
    "test_df = pipelineModel.transform(test_df)\n",
    "selectedCols = ['features'] + cols\n",
    "test_df = test_df.select(selectedCols)\n",
    "test_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_lr = lr_bestModel.transform(test_df)\n",
    "\n",
    "predictions_dt = dt_bestModel.transform(test_df)\n",
    "\n",
    "predictions_rf = rf_bestModel.transform(test_df)\n",
    "\n",
    "predictions_gbt = gbt_bestModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8158:> (1 + 2) / 3][Stage 8159:> (1 + 2) / 3][Stage 8160:=>(2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------------+-------------+-------------+--------------+\n",
      "|UserId|TrackId|lr_prediction|dt_prediction|rf_prediction|gbt_prediction|\n",
      "+------+-------+-------------+-------------+-------------+--------------+\n",
      "|199810| 208019|          0.0|          0.0|          0.0|           0.0|\n",
      "|199810|  74139|          0.0|          0.0|          0.0|           0.0|\n",
      "|199810|   9903|          0.0|          0.0|          0.0|           0.0|\n",
      "|199810| 242681|          0.0|          0.0|          0.0|           0.0|\n",
      "|199810|  18515|          1.0|          1.0|          1.0|           1.0|\n",
      "|199810| 105760|          1.0|          1.0|          1.0|           1.0|\n",
      "|199812| 276940|          0.0|          0.0|          0.0|           0.0|\n",
      "|199812| 142408|          1.0|          1.0|          1.0|           1.0|\n",
      "|199812| 130023|          1.0|          1.0|          1.0|           1.0|\n",
      "|199812|  29189|          0.0|          0.0|          0.0|           0.0|\n",
      "|199812| 223706|          1.0|          1.0|          1.0|           1.0|\n",
      "|199812| 211361|          0.0|          0.0|          0.0|           0.0|\n",
      "+------+-------+-------------+-------------+-------------+--------------+\n",
      "only showing top 12 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "predictions_lr = predictions_lr.withColumnRenamed(\n",
    "    \"prediction\", \"lr_prediction\")\n",
    "predictions_dt = predictions_dt.withColumnRenamed(\n",
    "    \"prediction\", \"dt_prediction\")\n",
    "predictions_rf = predictions_rf.withColumnRenamed(\n",
    "    \"prediction\", \"rf_prediction\")\n",
    "predictions_gbt = predictions_gbt.withColumnRenamed(\n",
    "    \"prediction\", \"gbt_prediction\")\n",
    "\n",
    "\n",
    "combined_df = predictions_lr.select(\"UserId\", \"TrackId\", \"lr_prediction\") \\\n",
    "    .join(predictions_dt.select(\"UserId\", \"TrackId\", \"dt_prediction\"), [\"UserId\", \"TrackId\"]) \\\n",
    "    .join(predictions_rf.select(\"UserId\", \"TrackId\", \"rf_prediction\"), [\"UserId\", \"TrackId\"]) \\\n",
    "    .join(predictions_gbt.select(\"UserId\", \"TrackId\", \"gbt_prediction\"), [\"UserId\", \"TrackId\"])\n",
    "\n",
    "combined_df.show(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------------+-------------+-------------+--------------+-----------------+-------------------+\n",
      "|UserId|TrackId|lr_prediction|dt_prediction|rf_prediction|gbt_prediction|         features|         prediction|\n",
      "+------+-------+-------------+-------------+-------------+--------------+-----------------+-------------------+\n",
      "|199810| 208019|          0.0|          0.0|          0.0|           0.0|        (4,[],[])|0.18853145010668956|\n",
      "|199810|  74139|          0.0|          0.0|          0.0|           0.0|        (4,[],[])|0.18853145010668956|\n",
      "|199810|   9903|          0.0|          0.0|          0.0|           0.0|        (4,[],[])|0.18853145010668956|\n",
      "|199810| 242681|          0.0|          0.0|          0.0|           0.0|        (4,[],[])|0.18853145010668956|\n",
      "|199810|  18515|          1.0|          1.0|          1.0|           1.0|[1.0,1.0,1.0,1.0]| 0.9291336473597179|\n",
      "|199810| 105760|          1.0|          1.0|          1.0|           1.0|[1.0,1.0,1.0,1.0]| 0.9291336473597179|\n",
      "|199812| 276940|          0.0|          0.0|          0.0|           0.0|        (4,[],[])|0.18853145010668956|\n",
      "|199812| 142408|          1.0|          1.0|          1.0|           1.0|[1.0,1.0,1.0,1.0]| 0.9291336473597179|\n",
      "|199812| 130023|          1.0|          1.0|          1.0|           1.0|[1.0,1.0,1.0,1.0]| 0.9291336473597179|\n",
      "|199812|  29189|          0.0|          0.0|          0.0|           0.0|        (4,[],[])|0.18853145010668956|\n",
      "|199812| 223706|          1.0|          1.0|          1.0|           1.0|[1.0,1.0,1.0,1.0]| 0.9291336473597179|\n",
      "|199812| 211361|          0.0|          0.0|          0.0|           0.0|        (4,[],[])|0.18853145010668956|\n",
      "|199813| 188441|          1.0|          1.0|          1.0|           1.0|[1.0,1.0,1.0,1.0]| 0.9291336473597179|\n",
      "|199813|  20968|          0.0|          0.0|          0.0|           0.0|        (4,[],[])|0.18853145010668956|\n",
      "|199813|  21571|          1.0|          1.0|          1.0|           1.0|[1.0,1.0,1.0,1.0]| 0.9291336473597179|\n",
      "|199813|  79640|          1.0|          1.0|          1.0|           1.0|[1.0,1.0,1.0,1.0]| 0.9291336473597179|\n",
      "|199813| 184173|          1.0|          1.0|          1.0|           1.0|[1.0,1.0,1.0,1.0]| 0.9291336473597179|\n",
      "|199813| 111874|          0.0|          0.0|          0.0|           0.0|        (4,[],[])|0.18853145010668956|\n",
      "|199814| 122375|          0.0|          0.0|          0.0|           0.0|        (4,[],[])|0.18853145010668956|\n",
      "|199814| 189043|          1.0|          1.0|          1.0|           1.0|[1.0,1.0,1.0,1.0]| 0.9291336473597179|\n",
      "+------+-------+-------------+-------------+-------------+--------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=[\n",
    "                            \"lr_prediction\", \"dt_prediction\", \"rf_prediction\", \"gbt_prediction\"], outputCol=\"features\")\n",
    "\n",
    "# Use the assembler to transform our DataFrame to the two-column format\n",
    "df = assembler.transform(combined_df)\n",
    "df = ensemble_lr.transform(df)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------------+-------------+-------------+--------------+-----------------+-------------------+------------+\n",
      "|UserId|TrackId|lr_prediction|dt_prediction|rf_prediction|gbt_prediction|         features|         prediction|final_result|\n",
      "+------+-------+-------------+-------------+-------------+--------------+-----------------+-------------------+------------+\n",
      "|199810|  18515|          1.0|          1.0|          1.0|           1.0|[1.0,1.0,1.0,1.0]| 0.9291336473597179|           1|\n",
      "|199810| 105760|          1.0|          1.0|          1.0|           1.0|[1.0,1.0,1.0,1.0]| 0.9291336473597179|           1|\n",
      "|199810| 208019|          0.0|          0.0|          0.0|           0.0|        (4,[],[])|0.18853145010668956|           1|\n",
      "|199810|  74139|          0.0|          0.0|          0.0|           0.0|        (4,[],[])|0.18853145010668956|           0|\n",
      "|199810|   9903|          0.0|          0.0|          0.0|           0.0|        (4,[],[])|0.18853145010668956|           0|\n",
      "|199810| 242681|          0.0|          0.0|          0.0|           0.0|        (4,[],[])|0.18853145010668956|           0|\n",
      "+------+-------+-------------+-------------+-------------+--------------+-----------------+-------------------+------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, when\n",
    "\n",
    "window = Window.partitionBy('UserId').orderBy(df['prediction'].desc())\n",
    "df = df.withColumn('row_num', row_number().over(window))\n",
    "df = df.withColumn('final_result', when(df['row_num'] <= 3, 1).otherwise(0))\n",
    "df = df.drop('row_num')\n",
    "\n",
    "df.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "df = df.withColumn('UserId_TrackId',concat(col('UserId'), lit('_'), col('TrackId')))\n",
    "\n",
    "df = df.select('TrackID', 'final_result')\n",
    "\n",
    "df = df.withColumnRenamed('final_result', 'Predictor')\n",
    "\n",
    "df.write.mode('overwrite').csv('submission.csv', header=True)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

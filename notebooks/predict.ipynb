{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/05/10 18:10:04 WARN Utils: Your hostname, BryanDesktop resolves to a loopback address: 127.0.1.1; using 172.30.8.97 instead (on interface eth0)\n",
      "23/05/10 18:10:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/10 18:10:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/10 18:10:15 WARN TaskSetManager: Stage 0 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded: 120000\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- trackId: integer (nullable = true)\n",
      " |-- albumId: integer (nullable = true)\n",
      " |-- artistId: integer (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+------+-------+-------+--------+--------------------+\n",
      "|userId|trackId|albumId|artistId|              genres|\n",
      "+------+-------+-------+--------+--------------------+\n",
      "|199810| 208019| 209288|    null|                  []|\n",
      "|199810|  74139| 277282|  271146|[113360, 173467, ...|\n",
      "|199810|   9903|   null|    null|[33722, 123396, 7...|\n",
      "|199810| 242681| 190640|  244574|[61215, 17453, 27...|\n",
      "|199810|  18515| 146344|   33168|[19913, 48505, 15...|\n",
      "+------+-------+-------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "als_model_dir = \"../models/ALS\"\n",
    "test_hierarchy_dir = '../data/testTrack_hierarchy.txt'\n",
    "\n",
    "spark = SparkSession.builder.appName('recommend-ML').getOrCreate()\n",
    "\n",
    "# Read the track data line by line\n",
    "with open(test_hierarchy_dir, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "track_data = []\n",
    "for line in lines:\n",
    "    fields = line.strip().split('|')\n",
    "    # Use the second number as the trackId\n",
    "    track_data.append([fields[0]] + fields[1:])\n",
    "\n",
    "# Create a DataFrame from the track data\n",
    "track_data_df = pd.DataFrame(track_data)\n",
    "\n",
    "# Fill NaN values with an empty string\n",
    "track_data_df.fillna(\"\", inplace=True)\n",
    "\n",
    "# Determine the maximum number of genres\n",
    "max_genres = track_data_df.shape[1] - 4\n",
    "\n",
    "# Rename columns\n",
    "track_data_df.columns = ['userId', 'trackId', 'albumId', 'artistId'] + \\\n",
    "    [f'genreId_{i}' for i in range(1, max_genres + 1)]\n",
    "\n",
    "track_data_df['trackId'] = pd.to_numeric(\n",
    "    track_data_df['trackId'], errors='coerce')\n",
    "track_data_df.dropna(subset=['trackId'], inplace=True)\n",
    "\n",
    "track_data_df['trackId'] = track_data_df['trackId'].astype(int)\n",
    "\n",
    "merged_df = track_data_df\n",
    "\n",
    "# Convert genres to a list of genres for each track\n",
    "merged_df['genres'] = merged_df[[\n",
    "    f'genreId_{i}' for i in range(1, max_genres + 1)]].values.tolist()\n",
    "\n",
    "# Remove empty strings from the genre lists\n",
    "merged_df['genres'] = merged_df['genres'].apply(\n",
    "    lambda x: [genre for genre in x if genre != \"\"])\n",
    "\n",
    "# Drop individual genre columns and itemId column\n",
    "merged_df.drop(columns=[f'genreId_{i}' for i in range(\n",
    "    1, max_genres + 1)], inplace=True)\n",
    "\n",
    "# Convert the Pandas DataFrame to a Spark DataFrame\n",
    "predict_df = spark.createDataFrame(merged_df)\n",
    "\n",
    "predict_df = predict_df.withColumn(\"albumId\", col(\"albumId\").cast(\"integer\"))\n",
    "predict_df = predict_df.withColumn(\"trackId\", col(\"trackId\").cast(\"integer\"))\n",
    "predict_df = predict_df.withColumn(\"userId\", col(\"userId\").cast(\"integer\"))\n",
    "predict_df = predict_df.withColumn(\n",
    "    \"artistId\", col(\"artistId\").cast(\"integer\"))\n",
    "\n",
    "print(\"loaded:\", predict_df.count())\n",
    "predict_df.printSchema()\n",
    "# Show the Spark DataFrame\n",
    "predict_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/10 18:10:16 WARN TaskSetManager: Stage 4 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:18 WARN TaskSetManager: Stage 8 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:21 WARN TaskSetManager: Stage 11 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:21 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/05/10 18:10:22 WARN TaskSetManager: Stage 14 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:22 WARN TaskSetManager: Stage 17 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:22 WARN TaskSetManager: Stage 20 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:22 WARN TaskSetManager: Stage 23 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:23 WARN TaskSetManager: Stage 26 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:23 WARN TaskSetManager: Stage 29 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:23 WARN TaskSetManager: Stage 32 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:23 WARN TaskSetManager: Stage 35 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:23 WARN TaskSetManager: Stage 38 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:24 WARN TaskSetManager: Stage 41 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:24 WARN TaskSetManager: Stage 44 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:24 WARN TaskSetManager: Stage 47 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:24 WARN TaskSetManager: Stage 50 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:24 WARN TaskSetManager: Stage 53 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:24 WARN TaskSetManager: Stage 56 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:24 WARN TaskSetManager: Stage 59 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:25 WARN TaskSetManager: Stage 62 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:25 WARN TaskSetManager: Stage 65 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:25 WARN TaskSetManager: Stage 68 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/05/10 18:10:25 WARN TaskSetManager: Stage 70 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded: 120000\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- trackId: integer (nullable = true)\n",
      " |-- albumId: integer (nullable = true)\n",
      " |-- artistId: integer (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ohe_features: vector (nullable = true)\n",
      " |-- topicDistribution: vector (nullable = true)\n",
      "\n",
      "None\n",
      "+------+-------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|userId|trackId|albumId|artistId|              genres|        ohe_features|   topicDistribution|\n",
      "+------+-------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|199810| 208019| 209288|    null|                  []|         (204,[],[])|[0.0,0.0,0.0,0.0,...|\n",
      "|199810|  74139| 277282|  271146|[113360, 173467, ...|(204,[3,4,10,22,5...|[0.12608497166443...|\n",
      "|199810|   9903|   null|    null|[33722, 123396, 7...|(204,[13,46,55,90...|[0.03630426225288...|\n",
      "|199810| 242681| 190640|  244574|[61215, 17453, 27...|(204,[0,7,8],[1.0...|[0.04552321789074...|\n",
      "|199810|  18515| 146344|   33168|[19913, 48505, 15...|(204,[4,15,38],[1...|[0.04552329533070...|\n",
      "|199810| 105760|  93458|   11616|[131552, 173467, ...|(204,[1,3,4,10],[...|[0.03641502746206...|\n",
      "|199812| 276940| 201356|  163237|            [287681]|    (204,[42],[1.0])|[0.09249064714592...|\n",
      "|199812| 142408| 112725|  275191|[158282, 173467, ...|(204,[3,4,6,10,11...|[0.53386557645656...|\n",
      "|199812| 130023| 226816|  275191|[158282, 242383, ...|(204,[6,11,15,31]...|[0.64333658820199...|\n",
      "|199812|  29189| 104694|  211701|[158282, 173467, ...|(204,[3,4,6,10,21...|[0.53298905259617...|\n",
      "+------+-------+-------+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"genres\", outputCol=\"ohe_features\")\n",
    "cv_model = cv.fit(predict_df)\n",
    "df_ohe = cv_model.transform(predict_df)\n",
    "\n",
    "\n",
    "num_topics = 5  # Choose the number of topics based on the desired lower dimensionality\n",
    "lda = LDA(k=num_topics, featuresCol=\"ohe_features\")\n",
    "lda_model = lda.fit(df_ohe)\n",
    "\n",
    "# Get the genre score for each row\n",
    "genre_scores = lda_model.transform(df_ohe)\n",
    "print(\"loaded:\", predict_df.count())\n",
    "print(genre_scores.printSchema())\n",
    "print(genre_scores.show(10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/10 18:10:26 WARN TaskSetManager: Stage 74 contains a task of very large size (1009 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded: 120000\n",
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- trackId: integer (nullable = true)\n",
      " |-- albumId: integer (nullable = true)\n",
      " |-- artistId: integer (nullable = true)\n",
      " |-- genres: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ohe_features: vector (nullable = true)\n",
      " |-- topicDistribution: vector (nullable = true)\n",
      "\n",
      "+--------------------+------+-------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|            features|userId|trackId|albumId|artistId|              genres|        ohe_features|   topicDistribution|\n",
      "+--------------------+------+-------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|(8,[0,1],[209288....|199810| 208019| 209288|       0|                  []|         (204,[],[])|[0.0,0.0,0.0,0.0,...|\n",
      "|[277282.0,74139.0...|199810|  74139| 277282|  271146|[113360, 173467, ...|(204,[3,4,10,22,5...|[0.12608497166443...|\n",
      "|[0.0,9903.0,0.0,0...|199810|   9903|      0|       0|[33722, 123396, 7...|(204,[13,46,55,90...|[0.03630426225288...|\n",
      "|[190640.0,242681....|199810| 242681| 190640|  244574|[61215, 17453, 27...|(204,[0,7,8],[1.0...|[0.04552321789074...|\n",
      "|[146344.0,18515.0...|199810|  18515| 146344|   33168|[19913, 48505, 15...|(204,[4,15,38],[1...|[0.04552329533070...|\n",
      "|[93458.0,105760.0...|199810| 105760|  93458|   11616|[131552, 173467, ...|(204,[1,3,4,10],[...|[0.03641502746206...|\n",
      "|[201356.0,276940....|199812| 276940| 201356|  163237|            [287681]|    (204,[42],[1.0])|[0.09249064714592...|\n",
      "|[112725.0,142408....|199812| 142408| 112725|  275191|[158282, 173467, ...|(204,[3,4,6,10,11...|[0.53386557645656...|\n",
      "|[226816.0,130023....|199812| 130023| 226816|  275191|[158282, 242383, ...|(204,[6,11,15,31]...|[0.64333658820199...|\n",
      "|[104694.0,29189.0...|199812|  29189| 104694|  211701|[158282, 173467, ...|(204,[3,4,6,10,21...|[0.53298905259617...|\n",
      "|[101750.0,223706....|199812| 223706| 101750|  128069|[158282, 207648, ...|(204,[6,16,31],[1...|[0.80723262728881...|\n",
      "|[127464.0,211361....|199812| 211361| 127464|   19438|             [77904]|    (204,[70],[1.0])|[0.60770510891675...|\n",
      "|[195380.0,188441....|199813| 188441| 195380|   22935|     [33722, 224280]|(204,[13,17],[1.0...|[0.06101295309237...|\n",
      "|[0.0,20968.0,0.0,...|199813|  20968|      0|       0|[61215, 34486, 27...|(204,[0,8,9,20,26...|[0.01804746661788...|\n",
      "|[217069.0,21571.0...|199813|  21571| 217069|   89183|             [71345]|   (204,[136],[1.0])|[0.09250559447123...|\n",
      "|[43214.0,79640.0,...|199813|  79640|  43214|  232859|[61215, 199606, 1...|(204,[0,2,5,25,92...|[0.03019254691630...|\n",
      "|[292343.0,184173....|199813| 184173| 292343|  196404|[33722, 271945, 2...|(204,[13,17,47],[...|[0.04552097375729...|\n",
      "|[117381.0,111874....|199813| 111874| 117381|  231280|[131552, 199606, ...|(204,[1,5,83],[1....|[0.04553073569352...|\n",
      "|[99568.0,122375.0...|199814| 122375|  99568|  105364|[214765, 131552, ...|(204,[1,12,41,61,...|[0.03019632554516...|\n",
      "|[9481.0,189043.0,...|199814| 189043|   9481|  247092|[61215, 17453, 25...|(204,[0,2,7,52,80...|[0.03020083607195...|\n",
      "+--------------------+------+-------+-------+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = genre_scores\n",
    "\n",
    "# Fill missing values with 0\n",
    "df = df.fillna(0, subset=['albumId', 'trackId', 'artistId'])\n",
    "\n",
    "stages = []\n",
    "numericCols = ['albumId', 'trackId', 'artistId']\n",
    "assemblerInputs = numericCols + ['topicDistribution']\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]\n",
    "\n",
    "#apply\n",
    "cols = df.columns\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df = pipelineModel.transform(df)\n",
    "selectedCols = ['features'] + cols\n",
    "df = df.select(selectedCols)\n",
    "print(\"loaded:\", df.count())\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|userId|trackId|\n",
      "+------+-------+\n",
      "|199810| 208019|\n",
      "|199810|  74139|\n",
      "|199810|   9903|\n",
      "|199810| 242681|\n",
      "|199810|  18515|\n",
      "+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/home/kyritzb/ai/music_recommendation/notebooks/test.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m loaded_als_model \u001b[38;5;241m=\u001b[39m ALSModel\u001b[38;5;241m.\u001b[39mload(als_model_dir)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Load the CSV file into a DataFrame\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m oldtest \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m oldtest \u001b[38;5;241m=\u001b[39m oldtest\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malbumId\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malbumId\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minteger\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     16\u001b[0m oldtest \u001b[38;5;241m=\u001b[39m oldtest\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrackId\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrackId\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minteger\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py:300\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py?line=297'>298</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py?line=298'>299</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py?line=299'>300</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mload(path))\n\u001b[1;32m    <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py?line=300'>301</a>\u001b[0m \u001b[39melif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py?line=301'>302</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/py4j/java_gateway.py?line=1315'>1316</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/py4j/java_gateway.py?line=1316'>1317</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/py4j/java_gateway.py?line=1317'>1318</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/py4j/java_gateway.py?line=1318'>1319</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/py4j/java_gateway.py?line=1320'>1321</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/py4j/java_gateway.py?line=1321'>1322</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/py4j/java_gateway.py?line=1322'>1323</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/py4j/java_gateway.py?line=1324'>1325</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/py4j/java_gateway.py?line=1325'>1326</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py?line=170'>171</a>\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py?line=171'>172</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py?line=172'>173</a>\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py?line=173'>174</a>\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py?line=174'>175</a>\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py?line=175'>176</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/kyritzb/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py?line=176'>177</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/home/kyritzb/ai/music_recommendation/notebooks/test."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALSModel\n",
    "\n",
    "df_user_item = df.select(\"userId\", \"trackId\")\n",
    "\n",
    "df_user_item.show(5)\n",
    "\n",
    "\n",
    "# Load the saved ALS model from the specified path\n",
    "loaded_als_model = ALSModel.load(als_model_dir)\n",
    "\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "oldtest = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"test\")\n",
    "\n",
    "oldtest = oldtest.withColumn(\"albumId\", col(\"albumId\").cast(\"integer\"))\n",
    "oldtest = oldtest.withColumn(\"trackId\", col(\"trackId\").cast(\"integer\"))\n",
    "oldtest = oldtest.withColumn(\"userId\", col(\"userId\").cast(\"integer\"))\n",
    "\n",
    "oldtest = oldtest.drop(\"rating\")\n",
    "oldtest = oldtest.drop(\"albumId\")\n",
    "oldtest = oldtest.drop(\"artistId\")\n",
    "oldtest = oldtest.drop(\"genres\")\n",
    "oldtest = oldtest.drop(\"features_str\")\n",
    "\n",
    "oldtest.printSchema()\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = loaded_als_model.transform(oldtest)\n",
    "predictions.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 312:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded: 6388958\n",
      "root\n",
      " |-- trackId: integer (nullable = true)\n",
      " |-- albumId: integer (nullable = true)\n",
      " |-- artistId: string (nullable = true)\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- features_str: string (nullable = true)\n",
      " |-- prediction: float (nullable = false)\n",
      " |-- final_prediction: double (nullable = false)\n",
      "\n",
      "+-------+-------+--------+------+--------------------+--------------------+----------+----------------+\n",
      "|trackId|albumId|artistId|userId|              genres|        features_str|prediction|final_prediction|\n",
      "+-------+-------+--------+------+--------------------+--------------------+----------+----------------+\n",
      "| 246668|     35|  145948|200283|                  []|[35.0,246668.0,14...| 13.953331|             0.0|\n",
      "| 236263|     49|   61215|202097|['172023', '18269...|[49.0,236263.0,61...| 10.934191|             0.0|\n",
      "| 236263|     49|   61215|203637|['172023', '18269...|[49.0,236263.0,61...| 38.624283|             0.0|\n",
      "| 236263|     49|   61215|204469|['172023', '18269...|[49.0,236263.0,61...|  33.57938|             0.0|\n",
      "|  11328|     98|  131552|205082|                  []|[98.0,11328.0,131...|  50.24372|             1.0|\n",
      "|  26940|     98|  131552|200715|                  []|[98.0,26940.0,131...|  6.178597|             0.0|\n",
      "|  26940|     98|  131552|204409|                  []|[98.0,26940.0,131...|  4.436098|             0.0|\n",
      "|  79025|     98|   82378|200336|                  []|[98.0,79025.0,823...| 0.8108509|             0.0|\n",
      "|  79025|     98|   82378|201153|                  []|[98.0,79025.0,823...| 14.228643|             0.0|\n",
      "|  79025|     98|   82378|201380|                  []|[98.0,79025.0,823...| 23.432201|             0.0|\n",
      "|  79025|     98|   82378|202888|                  []|[98.0,79025.0,823...| 6.1042986|             0.0|\n",
      "|  79025|     98|   82378|204409|                  []|[98.0,79025.0,823...| 2.3222647|             0.0|\n",
      "|  79025|     98|   82378|204549|                  []|[98.0,79025.0,823...| 0.6150311|             0.0|\n",
      "|  79025|     98|   82378|205053|                  []|[98.0,79025.0,823...| 4.3923197|             0.0|\n",
      "| 180892|    136|   61215|200999|                  []|[136.0,180892.0,6...|       0.0|             0.0|\n",
      "| 180892|    136|   61215|201903|                  []|[136.0,180892.0,6...|       0.0|             0.0|\n",
      "| 180892|    136|   61215|201903|                  []|[136.0,180892.0,6...|       0.0|             0.0|\n",
      "| 180892|    136|   61215|202644|                  []|[136.0,180892.0,6...|       0.0|             0.0|\n",
      "| 180892|    136|   61215|202644|                  []|[136.0,180892.0,6...|       0.0|             0.0|\n",
      "| 180892|    136|   61215|203431|                  []|[136.0,180892.0,6...| 0.4631447|             0.0|\n",
      "+-------+-------+--------+------+--------------------+--------------------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert the predicted ratings to binary values (0 or 1) based on a threshold\n",
    "threshold = 50\n",
    "predictions = predictions.withColumn(\n",
    "    \"final_prediction\", (predictions[\"prediction\"] >= threshold).cast(\"double\"))\n",
    "\n",
    "print(\"loaded:\", predictions.count())\n",
    "predictions.printSchema()\n",
    "predictions.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m concat_ws, col\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Add a new column with combined userId and trackId\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredictions\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muserId_trackId\u001b[39m\u001b[38;5;124m\"\u001b[39m, concat_ws(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muserId\u001b[39m\u001b[38;5;124m\"\u001b[39m), col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrackId\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Select required columns\u001b[39;00m\n\u001b[1;32m      9\u001b[0m output_df \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muserId_trackId\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_prediction\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws, col\n",
    "# Add a new column with combined userId and trackId\n",
    "predictions = predictions.withColumn(\"userId_trackId\", concat_ws(\n",
    "    \"_\", col(\"userId\"), col(\"trackId\")))\n",
    "\n",
    "\n",
    "\n",
    "# Select required columns\n",
    "output_df = predictions.select(\"userId_trackId\", \"final_prediction\")\n",
    "\n",
    "output_df = output_df.withColumn(\n",
    "    \"final_prediction\", col(\"final_prediction\").cast(\"integer\"))\n",
    "\n",
    "# Reduce the number of partitions to one\n",
    "output_df = output_df.coalesce(1)\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "output_df.write.mode(\"overwrite\").csv(\"../results/als_output\", header=True)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
